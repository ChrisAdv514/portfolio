{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uudynCQPhIL_"
      },
      "source": [
        "#### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5BS_mNHb9m-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# must be imported to google colab L4 GPU\n",
        "!pip install bitsandbytes\n",
        "!pip install hf-xet\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        ")\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sns\n",
        "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
        "from pathlib import Path\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IzLL4CoiDT0"
      },
      "source": [
        "#### 1/ Configurations and storing the collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM_GhB04uG2I"
      },
      "outputs": [],
      "source": [
        "# collections and token for llama\n",
        "COLLECTION_PATH = 'project_collection_2024_25.json' # change path to test diff json\n",
        "\n",
        "HF_TOKEN = \"YOUR_HF_API_TOKEN_HERE\"\n",
        "# personal token for 3.1--8B-instruct 3 models NO GO with current setup\n",
        "# HF_TOKEN2   = token_path.read_text().strip()\n",
        "\n",
        "# models used can be changed accordingly\n",
        "BERT_MODELS = {\n",
        "    \"roberta\": \"deepset/roberta-base-squad2\",\n",
        "    \"distilbert\": \"distilbert-base-cased-distilled-squad\",\n",
        "    \"bert-small\": \"mrm8488/bert-small-finetuned-squadv2\",\n",
        "    \"bert-large\": \"deepset/roberta-large-squad2\",\n",
        "}\n",
        "LLAMA_MODEL = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-3.2-1B-Instruct  3 MODEL NOGO with the prompt below\n",
        "DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# seed for reproducibility\n",
        "SEED = 2\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# get the collection strings {id, title text, and list of qa}\n",
        "@dataclass\n",
        "class QA: # define qa\n",
        "    question: str\n",
        "    answer: str\n",
        "    type: Optional[str] = None\n",
        "    entity: Optional[str] = None\n",
        "@dataclass\n",
        "class Topic:\n",
        "    id: str\n",
        "    title: str\n",
        "    text: str\n",
        "    qa_list: List[QA]\n",
        "\n",
        "\n",
        "def load_collection(path: str|Path) -> List[Topic]:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = json.load(f)\n",
        "\n",
        "    topics=[]\n",
        "    for entry in raw_data:\n",
        "        # either 'text' or 'context' in topics\n",
        "        content = entry.get('text') or entry.get('context')\n",
        "\n",
        "        qa_items = []\n",
        "        for qa_dict in entry.get('qa', []):\n",
        "            question_text = qa_dict.get('question', \"\").strip()\n",
        "            answer_text = qa_dict.get('answer',  \"\").strip()\n",
        "            q_type = qa_dict.get('type')\n",
        "            q_entity = qa_dict.get('entity')\n",
        "\n",
        "            # append to qa\n",
        "            qa_items.append(QA(\n",
        "                question=question_text,\n",
        "                answer=answer_text,\n",
        "                type=q_type,\n",
        "                entity=q_entity\n",
        "            ))\n",
        "\n",
        "\n",
        "        # topic to list\n",
        "        topic_id = str(entry.get('id', \"\"))\n",
        "        topic_title = entry.get('title', \"\").strip()\n",
        "\n",
        "        # everything to topics\n",
        "        topics.append(Topic(\n",
        "            id=topic_id,\n",
        "            title=topic_title,\n",
        "            text=content,\n",
        "            qa_list=qa_items\n",
        "        ))\n",
        "\n",
        "    return topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yxyoveqLFx6"
      },
      "source": [
        "#### 2 /Basic QA system with random word selection and evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o34R2AxqLKDK"
      },
      "outputs": [],
      "source": [
        "# return one random word from text as baseline\n",
        "def random_word(text: str) -> str:\n",
        "\n",
        "    # find all words from the text\n",
        "    words = re.compile(r\"\\b\\w+\\b\").findall(text)\n",
        "\n",
        "    # return random\n",
        "    return random.choice(words)\n",
        "\n",
        "# exact match and token F1 score\n",
        "def calculate_em_f1(prediction: str, reference: str) -> Tuple[int, float]:\n",
        "    # lowercase and strip whitespace\n",
        "    pred_norm = prediction.strip().lower()\n",
        "    gold_norm = reference.strip().lower()\n",
        "\n",
        "    # exact match\n",
        "    em_score = int(pred_norm == gold_norm)\n",
        "\n",
        "    # tokenize on whitespace\n",
        "    pred_tokens = pred_norm.split()\n",
        "    gold_tokens = gold_norm.split()\n",
        "\n",
        "    # count common tokens\n",
        "    common_tokens = Counter(pred_tokens) & Counter(gold_tokens)\n",
        "    num_common = sum(common_tokens.values())\n",
        "\n",
        "    # If either has zero tokens, F1 is 0\n",
        "    if not pred_tokens or not gold_tokens:\n",
        "        return em_score, 0.0\n",
        "\n",
        "    # precision and recall\n",
        "    precision = num_common / len(pred_tokens)\n",
        "    recall = num_common / len(gold_tokens)\n",
        "\n",
        "    # F1\n",
        "    if precision + recall == 0:\n",
        "        f1_score = 0.0\n",
        "    else:\n",
        "        f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return em_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhOyHTLTuPgq"
      },
      "source": [
        "### Core functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0HvFAvCj9Ry"
      },
      "source": [
        "#### 3/ Question type and entity type prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyZg6z2okFbQ"
      },
      "outputs": [],
      "source": [
        "class QuestionClassifier: # what worked best in assignment2 is BERT+SVM or RNFOREST we can also do tfidf but results are about 42% acc\n",
        "    # factoid vs confirmation\n",
        "    def __init__(self, sentence_embedder: SentenceTransformer):\n",
        "        self.sentence_embedder = sentence_embedder # Store the embedder instance\n",
        "        # classifier for questions\n",
        "        self.type_classifier = RandomForestClassifier(random_state=SEED)\n",
        "        # classifier for entities on factoids\n",
        "        self.entity_classifier = RandomForestClassifier(random_state=SEED)\n",
        "\n",
        "    # train both classifiers\n",
        "    def train(self, qa_items: List[QA]) -> None:\n",
        "        # get texts and labels\n",
        "        questions = [item.question for item in qa_items]\n",
        "        types = [item.type     for item in qa_items]\n",
        "\n",
        "        # SentenceTransformer to get embeddings directly returns the feature matrix\n",
        "        feature_matrix = self.sentence_embedder.encode(questions)\n",
        "\n",
        "        self.type_classifier.fit(feature_matrix, types)\n",
        "\n",
        "        # get indices of factoid questions\n",
        "        factoid_idxs = [i for i, t in enumerate(types) if t == \"factoid\"]\n",
        "        if factoid_idxs:\n",
        "            # get features and entities for factoid questions\n",
        "            factoid_features = feature_matrix[factoid_idxs]\n",
        "            factoid_entities = [qa_items[i].entity for i in factoid_idxs]\n",
        "            self.entity_classifier.fit(factoid_features, factoid_entities)\n",
        "\n",
        "\n",
        "    # predict type of question\n",
        "    def predict(self, question: str) -> Tuple[str, Optional[str]]:\n",
        "        features = self.sentence_embedder.encode([question]) # Encode the single question\n",
        "\n",
        "        # predict question type\n",
        "        q_type = self.type_classifier.predict(features)[0]\n",
        "\n",
        "        # if factoid predict the entity type\n",
        "        entity = (\n",
        "            self.entity_classifier.predict(features)[0]\n",
        "            if q_type == \"factoid\" else\n",
        "            None\n",
        "        )\n",
        "        return q_type, entity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poUct3iWkPiB"
      },
      "source": [
        "#### 4/ Implement QA with different transformer models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCfhN450kQ1J"
      },
      "outputs": [],
      "source": [
        "# load int a streamlined pipeline all BERT models\n",
        "class BERTpipeline:\n",
        "    def __init__(self, device: int):\n",
        "        self.pipelines: Dict[str, \"transformers.Pipeline\"] = {\n",
        "            name: pipeline(\n",
        "                task = \"question-answering\",\n",
        "                model = path,\n",
        "                tokenizer = path,\n",
        "                device = device,\n",
        "                # token = HF_TOKEN2 # can deactivate\n",
        "            )\n",
        "            for name, path in BERT_MODELS.items()\n",
        "        }\n",
        "\n",
        "    # get answer/confidence/latency for each model\n",
        "    def answer(self, question: str, context: str) -> Dict[str, Tuple[str, float, float]]:\n",
        "        res = {}\n",
        "        for model_name, qa_pipe in self.pipelines.items():\n",
        "            # get time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # run the pipeline for each model\n",
        "            output = qa_pipe(question=question, context=context)\n",
        "\n",
        "            # time stop\n",
        "            latency_seconds = time.time() - start_time\n",
        "\n",
        "            # extract and clean lowercase and strip whitespace\n",
        "            answer_text = output.get(\"answer\", \"\").strip()\n",
        "            confidence_score = output.get(\"score\", 0.0)\n",
        "\n",
        "            # store dict\n",
        "            res[model_name] = (answer_text, confidence_score, latency_seconds)\n",
        "\n",
        "        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XavINT-kQLi"
      },
      "source": [
        "#### 5/ Llama QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ugEs1SJkRJc"
      },
      "outputs": [],
      "source": [
        "LLAMA_QA_PROMPT = (\n",
        "    \"\"\"<s>[INST] <<SYS>>\n",
        "    You are an AI that answers questions from texts.\n",
        "    <</SYS>>\n",
        "    Given the following report, answer the question with the shortest possible phrase.\n",
        "    ## REPORT\n",
        "    {context}\n",
        "    ## QUESTION\n",
        "    {question} [/INST]\n",
        "    ##ANSWER\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "class LlamaModelBase:\n",
        "    def __init__(self, model_path: str, hf_token: str | None, device: int):\n",
        "        self.enabled = torch.cuda.is_available() and bool(hf_token)\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "            bnb_4bit_compute_dtype = torch.bfloat16\n",
        "        )\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_path,\n",
        "            token = hf_token\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            device_map = \"auto\",\n",
        "            quantization_config = quant_cfg,\n",
        "            token = hf_token, \n",
        "            trust_remote_code = True\n",
        "        ).eval()\n",
        "\n",
        "    def _generate_text(self, prompt: str, max_new_tokens: int) -> str:\n",
        "        if not self.enabled or self.model is None:\n",
        "            return \"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "        generated = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens = max_new_tokens,\n",
        "            do_sample = False,\n",
        "            eos_token_id = self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        output_text = self.tokenizer.decode(\n",
        "            generated[0],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        return output_text\n",
        "\n",
        "\n",
        "class LlamaQA(LlamaModelBase):\n",
        "    def __init__(self, hf_token: str | None, device: int):\n",
        "        super().__init__(LLAMA_MODEL, hf_token, device)\n",
        "\n",
        "    def answer(self, question: str, context: str, max_new_tokens: int = 128) -> str:\n",
        "        prompt_text = LLAMA_QA_PROMPT.format(context=context, question=question)\n",
        "        output_text = self._generate_text(prompt_text, max_new_tokens)\n",
        "\n",
        "        if \"##ANSWER:\" in output_text:\n",
        "            return output_text.split(\"##ANSWER:\")[-1].strip()\n",
        "        else:\n",
        "            lines = [line.strip() for line in output_text.splitlines() if line.strip()]\n",
        "            return lines[-1] if lines else \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQUOHy1hzm-T"
      },
      "source": [
        "#### 7/ Confirmation yer or no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFU6bgMrzj0i"
      },
      "outputs": [],
      "source": [
        "# cross-encoder to decide yes or no\n",
        "class ConfirmationAnswerer:\n",
        "    def __init__(self, threshold: float = 0.5, device: int = DEVICE):\n",
        "        self.threshold = threshold\n",
        "        self.cross_encoder = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\", device=device) #best for this task\n",
        "        # cross-encoder/qnli-distilroberta-base cross-encoder/ms-marco-MiniLM-L6-v2\n",
        "        # We need a separate sentence embedder for finding relevant sentences within a topic\n",
        "        self.sentence_embedder = None # will be passed from the evaluate function.\n",
        "\n",
        "    # Set the sentence embedder\n",
        "    def set_sentence_embedder(self, embedder: SentenceTransformer):\n",
        "        self.sentence_embedder = embedder\n",
        "\n",
        "    # predict\n",
        "    def answer(self, question: str, full_topic_text: str) -> str:\n",
        "        # split the full topic text into sentences\n",
        "        sentences = sent_tokenize(full_topic_text)\n",
        "        sentences = [s.strip() for s in sentences if s.strip()] # Clean empty sentences\n",
        "\n",
        "        if not sentences:\n",
        "            return \"No\" # No context so cant confirm\n",
        "\n",
        "        # Find the most relevant sentence\n",
        "        # reuse the approach from passageRetriever but at sentence level\n",
        "        if self.sentence_embedder:\n",
        "            question_embedding = self.sentence_embedder.encode([question])\n",
        "            sentence_embeddings = self.sentence_embedder.encode(sentences)\n",
        "\n",
        "            # calculate cosine similarity between question and each sentence\n",
        "            similarities = cosine_similarity(question_embedding, sentence_embeddings)[0]\n",
        "\n",
        "            # Get the top scoring sentence as context for the cross-encoder using top@1\n",
        "            best_sentence_idx = np.argmax(similarities)\n",
        "            context_for_cross_encoder = sentences[best_sentence_idx]\n",
        "        else: # if not used use full text\n",
        "            context_for_cross_encoder = full_topic_text\n",
        "\n",
        "        # cross encoder with the refined context\n",
        "        score = self.cross_encoder.predict([(question, context_for_cross_encoder)])[0]\n",
        "\n",
        "        # apply threshold\n",
        "        return \"Yes\" if score >= self.threshold else \"No\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_MSEwzKzp4a"
      },
      "source": [
        "#### 9/ Passage retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqvXJVthzvQX"
      },
      "outputs": [],
      "source": [
        "# rank a list of topics by relevance\n",
        "class PassageRetriever:\n",
        "    def __init__(self, device: int = DEVICE):\n",
        "        self.model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\", device=device) # best so far\n",
        "        # , cross-encoder/qnli-distilroberta-base\n",
        "    # select index of most relevant topic\n",
        "    def best_topic(self, question: str, topics: List[Topic]) -> int:\n",
        "        # pair each text/question of each topic\n",
        "        pairs = [(question, topic.text) for topic in topics]\n",
        "\n",
        "        # get relevance scores\n",
        "        scores = self.model.predict(pairs)\n",
        "\n",
        "        # get hishest\n",
        "        return int(np.argmax(scores))\n",
        "\n",
        "# majority vote of the predictions\n",
        "def majority_vote(predictions: List[str]) -> str:\n",
        "\n",
        "    # count answer and return the answer with the highest count\n",
        "    return Counter(predictions).most_common(1)[0][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5wPn1W0BlP"
      },
      "source": [
        "#### eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFhXJAXs0DF0"
      },
      "outputs": [],
      "source": [
        "def evaluate(topics: List[Topic],\n",
        "             hf_token: str | None = HF_TOKEN,\n",
        "             device: int = DEVICE,\n",
        "             show_progress: bool = True) -> None:\n",
        "\n",
        "    ##### init all reusable functions ####\n",
        "    BERTpipelines = BERTpipeline(device)\n",
        "    llama_qa = LlamaQA(hf_token, device)\n",
        "    passage_retriever = PassageRetriever()\n",
        "\n",
        "    # Init Sentence transformer confirmation/and question classifier\n",
        "    # Using a common, lightweight model for sentence embeddings\n",
        "    # The model used here is the most downloaded\n",
        "    sentence_embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=f\"cuda:{DEVICE}\" if DEVICE != -1 else \"cpu\") # best model\n",
        "    question_classifier = QuestionClassifier(sentence_embedder)\n",
        "    confirmation_model = ConfirmationAnswerer(device=device)\n",
        "    confirmation_model.set_sentence_embedder(sentence_embedder) # Pass the embeder to the confirmation model\n",
        "\n",
        "\n",
        "    # classifier training and evaluation with proper train/test for no data leakage\n",
        "    all_questions_list = [qa for topic in topics for qa in topic.qa_list]\n",
        "\n",
        "    # split all questions\n",
        "    train_classifier_qa_items, test_classifier_qa_items = train_test_split(\n",
        "        all_questions_list,\n",
        "        test_size=0.2,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    question_classifier.train(train_classifier_qa_items)\n",
        "\n",
        "    # eval the question classifier on test\n",
        "    q_type_correct_eval = 0\n",
        "    q_type_total_eval = 0\n",
        "    entity_correct_eval = 0\n",
        "    entity_total_eval = 0\n",
        "\n",
        "    print(\"Evaluating Question/Entity type classifier on test set\")\n",
        "    for qa_item in test_classifier_qa_items:\n",
        "        predicted_q_type, predicted_entity = question_classifier.predict(qa_item.question)\n",
        "        q_type_total_eval += 1\n",
        "        if predicted_q_type == qa_item.type:\n",
        "            q_type_correct_eval += 1\n",
        "\n",
        "        if qa_item.type == \"factoid\":\n",
        "            entity_total_eval += 1\n",
        "            if predicted_entity == qa_item.entity:\n",
        "                entity_correct_eval += 1\n",
        "\n",
        "    # Eval to find the optimal threshold for confirmation answerer\n",
        "    thresholds = np.arange(0.1, 1.0, 0.05) # test thresholds from 0.1 to 0.95\n",
        "    best_confirm_accuracy = -1\n",
        "    optimal_threshold = 0.5\n",
        "    LOW_CONFIDENCE_THRESHOLD = 0.35 # for semantic re-rank\n",
        "\n",
        "    confirm_qa_items = [qa for qa in all_questions_list if qa.type == \"confirmation\"]\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        current_confirm_correct = 0.0\n",
        "        current_confirm_total = 0\n",
        "\n",
        "        temp_confirmation_model = ConfirmationAnswerer(threshold=threshold, device=device)\n",
        "        temp_confirmation_model.set_sentence_embedder(sentence_embedder) # Pass embedder\n",
        "\n",
        "        for qa_item in confirm_qa_items:\n",
        "            # Find the parent topic text for this qa_item\n",
        "            parent_topic_text = None\n",
        "            for topic in topics:\n",
        "                if qa_item in topic.qa_list:\n",
        "                    parent_topic_text = topic.text\n",
        "                    break\n",
        "\n",
        "            if parent_topic_text: # Ensure text is found\n",
        "                prediction = temp_confirmation_model.answer(qa_item.question, parent_topic_text)\n",
        "                current_confirm_correct += int(prediction.lower() == qa_item.answer.lower())\n",
        "                current_confirm_total += 1\n",
        "\n",
        "        if current_confirm_total > 0:\n",
        "            current_accuracy = current_confirm_correct / current_confirm_total\n",
        "            if current_accuracy > best_confirm_accuracy:\n",
        "                best_confirm_accuracy = current_accuracy\n",
        "                optimal_threshold = threshold\n",
        "\n",
        "    print(f\"Optimal confirmation threshold: {optimal_threshold:.2f} with accuracy: {best_confirm_accuracy:.2%}\")\n",
        "    confirmation_model.threshold = optimal_threshold\n",
        "\n",
        "    # running totals for every metric we will report\n",
        "    # EM_sum, F1_sum, Latency_sum, count\n",
        "    per_model_scores = defaultdict(lambda: [0.0, 0.0, 0.0, 0])\n",
        "    per_entity_scores = defaultdict(lambda: [0.0, 0.0, 0])\n",
        "\n",
        "\n",
        "    baseline_em_sum = 0.0\n",
        "    baseline_f1_sum = 0.0\n",
        "    baseline_count = 0\n",
        "\n",
        "    confirm_correct = 0.0\n",
        "    confirm_total = 0\n",
        "    retrieval_correct = 0.0\n",
        "    retrieval_total = 0\n",
        "\n",
        "\n",
        "    # Itr over every topic on every question\n",
        "    # track 40 topics\n",
        "    topic_iter = tqdm(enumerate(topics), desc=\"Topics\") if show_progress else enumerate(topics) # track it\n",
        "\n",
        "    for topic_index, topic in topic_iter:\n",
        "        for qa in topic.qa_list: # Iterate over ALL questions in the original collection\n",
        "\n",
        "            # Passage retrieval\n",
        "            retrieval_total += 1\n",
        "            if passage_retriever.best_topic(qa.question, topics) == topic_index:\n",
        "                retrieval_correct += 1\n",
        "\n",
        "            # Random word as baseline\n",
        "            random_guess = random_word(topic.text)\n",
        "            em, f1 = calculate_em_f1(random_guess, qa.answer)\n",
        "            baseline_em_sum += em\n",
        "            baseline_f1_sum += f1\n",
        "            baseline_count  += 1\n",
        "\n",
        "            # Confirmation of questions\n",
        "            if qa.type == \"confirmation\": # will now use refined context and optimal threshold\n",
        "                prediction = confirmation_model.answer(qa.question, topic.text)\n",
        "                confirm_correct += int(prediction.lower() == qa.answer.lower())\n",
        "                confirm_total += 1\n",
        "                continue  # no EM/F1 for confirmation questions for BERT/LLaMA\n",
        "\n",
        "            # BERT\n",
        "            bert_outputs = BERTpipelines.answer(qa.question, topic.text)\n",
        "            for tag, (answer, confidence, latency) in bert_outputs.items():\n",
        "                em, f1 = calculate_em_f1(answer, qa.answer)\n",
        "                agg = per_model_scores[tag]\n",
        "                per_model_scores[tag] = [agg[0] + em, agg[1] + f1, agg[2] + latency, agg[3] + 1]\n",
        "\n",
        "            # LLAMA-2\n",
        "            llama_start_time = time.time()\n",
        "            llama_answer = llama_qa.answer(qa.question, topic.text)\n",
        "            llama_latency = time.time() - llama_start_time\n",
        "            em, f1 = calculate_em_f1(llama_answer, qa.answer)\n",
        "            agg = per_model_scores[\"llama\"]\n",
        "            per_model_scores[\"llama\"] = [agg[0] + em, agg[1] + f1, agg[2] + llama_latency, agg[3] + 1]\n",
        "\n",
        "            ### ensemble of models for to better the perfomance\n",
        "            # choose the highest-confidence BERT answer + LLAMA vote\n",
        "            best_bert_answer = \"\"\n",
        "            best_bert_confidence = 0.0\n",
        "            if bert_outputs:\n",
        "                best_bert_tag, (best_bert_answer, best_bert_confidence, _) = max(bert_outputs.items(), key=lambda kv: kv[1][1])\n",
        "\n",
        "            ensemble_answer_orig = majority_vote([best_bert_answer, llama_answer or best_bert_answer])\n",
        "            em, f1 = calculate_em_f1(ensemble_answer_orig, qa.answer)\n",
        "            agg = per_model_scores[\"ensemble\"]\n",
        "            per_model_scores[\"ensemble\"] = [agg[0] + em, agg[1] + f1, agg[2], agg[3] + 1]\n",
        "\n",
        "            ### semantic re-ranking\n",
        "            semantic_rerank_answer = \"\"\n",
        "\n",
        "            if best_bert_confidence < LOW_CONFIDENCE_THRESHOLD:\n",
        "                candidate_answers = {best_bert_answer} # Use a set to get unique answers\n",
        "                if llama_answer:\n",
        "                    candidate_answers.add(llama_answer)\n",
        "\n",
        "                candidate_answers = [ans for ans in candidate_answers if ans.strip()]\n",
        "\n",
        "                if candidate_answers:\n",
        "                    question_embedding = sentence_embedder.encode([qa.question])\n",
        "                    candidate_embeddings = sentence_embedder.encode(candidate_answers)\n",
        "                    similarities = cosine_similarity(question_embedding, candidate_embeddings)[0]\n",
        "                    best_candidate_idx = np.argmax(similarities)\n",
        "                    semantic_rerank_answer = candidate_answers[best_candidate_idx]\n",
        "                else:\n",
        "                    semantic_rerank_answer = best_bert_answer\n",
        "            else:\n",
        "                semantic_rerank_answer = best_bert_answer\n",
        "\n",
        "            em, f1 = calculate_em_f1(semantic_rerank_answer, qa.answer)\n",
        "            agg = per_model_scores[\"ensemble_rerank\"]\n",
        "            per_model_scores[\"ensemble_rerank\"] = [agg[0] + em, agg[1] + f1, agg[2], agg[3] + 1]\n",
        "\n",
        "\n",
        "            # Entity level\n",
        "            if qa.entity:\n",
        "                # Use the answer from the highest-confidence BERT model for entity level\n",
        "                # Ensure BERT is not empty\n",
        "                if bert_outputs:\n",
        "                    entity_qa_answer = bert_outputs[max(bert_outputs.items(), key=lambda kv: kv[1][1])[0]][0]\n",
        "                else:\n",
        "                    entity_qa_answer = \"\"\n",
        "\n",
        "                em, f1 = calculate_em_f1(entity_qa_answer, qa.answer)\n",
        "                agg = per_entity_scores[qa.entity]\n",
        "                per_entity_scores[qa.entity] = [agg[0] + em, agg[1] + f1, agg[2] + 1]\n",
        "\n",
        "    ### Summary and visualization ###\n",
        "    best_overall_model_name = \"N/A\"\n",
        "    best_overall_model_f1 = 0.0\n",
        "    model_performance_data_temp = [] # Collect data to determine overall best model\n",
        "    for tag, (em_sum, f1_sum, latency_sum, n) in per_model_scores.items():\n",
        "        if n > 0:\n",
        "            avg_latency = latency_sum / n if latency_sum > 0 else 0\n",
        "            model_performance_data_temp.append({\n",
        "                \"Model\": tag,\n",
        "                \"EM\": em_sum / n,\n",
        "                \"F1\": f1_sum / n,\n",
        "                \"Avg Latency (s)\": avg_latency,\n",
        "                \"N_Questions\": n\n",
        "            })\n",
        "\n",
        "    df_models_all = pd.DataFrame(model_performance_data_temp).sort_values(by=\"F1\", ascending=False)\n",
        "    best_overall_model_name = df_models_all.iloc[0]['Model']\n",
        "    best_overall_model_f1 = df_models_all.iloc[0]['F1']\n",
        "    print(f\"Overall best performing model: {best_overall_model_name} (F1: {best_overall_model_f1:.2%})\")\n",
        "\n",
        "    # overall perfomance\n",
        "    print(\"Overall Performance\")\n",
        "    overall_data = {\n",
        "        \"Metric\": [\"Random-Word Baseline\", \"Question Type Prediction\", \"Factoid Entity Type Prediction\",\n",
        "                   \"Passage Retrieval Accuracy\"],\n",
        "        \"Value\": [\n",
        "            f\"{baseline_em_sum / baseline_count:.2%} EM / {baseline_f1_sum / baseline_count:.2%} F1\",\n",
        "            f\"{q_type_correct_eval / q_type_total_eval:.2%}\" if q_type_total_eval > 0 else \"N/A\",\n",
        "            f\"{entity_correct_eval / entity_total_eval:.2%}\" if entity_total_eval > 0 else \"N/A\",\n",
        "            f\"{retrieval_correct / retrieval_total:.2%}\"\n",
        "        ],\n",
        "\n",
        "        \"N_Questions\": [\n",
        "            baseline_count,\n",
        "            q_type_total_eval if q_type_total_eval > 0 else 0,\n",
        "            entity_total_eval if entity_total_eval > 0 else 0,\n",
        "            retrieval_total\n",
        "        ]\n",
        "    }\n",
        "    df_overall = pd.DataFrame(overall_data)\n",
        "    print(df_overall.to_string(index=False))\n",
        "\n",
        "\n",
        "    # confirmation question yes/no\n",
        "    print(\"Confirmation question performance\")\n",
        "    confirm_accuracy = confirm_correct / confirm_total\n",
        "    print(f\"Confirmation question accuracy using optimal threshold: {confirm_accuracy:.2%} (N={confirm_total})\")\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.barplot(x=['Confirmation Questions'], y=[confirm_accuracy])\n",
        "    plt.title('Confirmation question accuracy with optimal threshold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
        "    for p in plt.gca().patches:\n",
        "        plt.gca().annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    ## Per model performance\n",
        "    print(\"Per model performance for factoid questions only\")\n",
        "    model_performance_data = []\n",
        "    for tag, (em_sum, f1_sum, latency_sum, n) in per_model_scores.items():\n",
        "        if n > 0:\n",
        "            model_performance_data.append({\n",
        "                \"Model\": tag,\n",
        "                \"EM\": em_sum / n,\n",
        "                \"F1\": f1_sum / n,\n",
        "                \"Avg Latency (s)\": latency_sum / n,\n",
        "                \"N_Questions\": n\n",
        "            })\n",
        "    df_models = pd.DataFrame(model_performance_data).sort_values(by=\"F1\", ascending=False)\n",
        "    print(df_models.to_string(index=False, formatters={\"EM\": \"{:.2%}\".format, \"F1\": \"{:.2%}\".format, \"Avg Latency (s)\": \"{:.5f}\".format}))\n",
        "\n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    fig.suptitle('Per model performance: EM, F1-Score, and Average latency', fontsize=16)\n",
        "\n",
        "    # EM and F1 vis\n",
        "    df_melted_em_f1 = df_models.melt(id_vars=['Model'], value_vars=['EM', 'F1'], var_name='Metric type', value_name='Score')\n",
        "    sns.barplot(x='Model', y='Score', hue='Metric type', data=df_melted_em_f1, ax=axes[0])\n",
        "    axes[0].set_title('EM and F1')\n",
        "    axes[0].set_ylabel('Score')\n",
        "    axes[0].set_ylim(0, 1) # Scores are between 0 and 1\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    for p in axes[0].patches:\n",
        "        axes[0].annotate(f'{p.get_height():.3f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "\n",
        "    # plot avg latency\n",
        "    sns.barplot(x='Model', y='Avg Latency (s)', data=df_models.sort_values(by=\"Avg Latency (s)\", ascending=False), ax=axes[1])\n",
        "    axes[1].set_title('Average latency per model')\n",
        "    axes[1].set_ylabel('seconds')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    for p in axes[1].patches:\n",
        "        axes[1].annotate(f'{p.get_height():.3f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    ## Entity level performance of best model for each entity\n",
        "    print(\"Entity level performance best overall\")\n",
        "    entity_performance_data = []\n",
        "    for ent, (em_sum, f1_sum, n) in per_entity_scores.items():\n",
        "        if n > 0:\n",
        "            entity_performance_data.append({\n",
        "                \"Entity Type\": ent,\n",
        "                \"EM\": em_sum / n,\n",
        "                \"F1\": f1_sum / n,\n",
        "                \"N_Questions\": n\n",
        "            })\n",
        "    df_entities = pd.DataFrame(entity_performance_data).sort_values(by=\"F1\", ascending=False)\n",
        "    print(df_entities.to_string(index=False, formatters={\"EM\": \"{:.2%}\".format, \"F1\": \"{:.2%}\".format}))\n",
        "\n",
        "    # visualization\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Entity Type', y='F1', data=df_entities)\n",
        "    plt.title(f'F1 score per entity type from answers from best confidence BERT per question')\n",
        "    plt.ylabel('F1')\n",
        "    plt.xlabel('Entity type')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    for p in plt.gca().patches:\n",
        "        plt.gca().annotate(f'{p.get_height():.3f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfdu6i8Bk4PP"
      },
      "source": [
        "### Main code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6b19f67279c149819f0938485b1ef88e",
            "4da186d5e8fc4ad8a9f225826903a86e",
            "40c69dc4b2a34595aebf997528fe56fb",
            "195afe943110489f87e24fd9ef8d8ac0",
            "323bf4164fd6471c8e5cf48c610fa170",
            "c856dc10f12e459da10697816ea714f8",
            "d81e95ab245d4d51af0fcba0a2e29f6e",
            "efd5530494e24f04a4e1844e6199d5f7",
            "12464ec10e57494fb8b7f9cd8c64b4e8",
            "0fbd4b3094554506a3dfe6d4fd437f9d",
            "8404a4d2f50d43cba9892172d8c52da6"
          ]
        },
        "id": "XC3Nuez4k3_b",
        "outputId": "35feed5e-024e-4b5f-9ad1-6f45893ce25b"
      },
      "outputs": [],
      "source": [
        "topics = load_collection(COLLECTION_PATH)\n",
        "evaluate(topics = topics, hf_token = HF_TOKEN)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fbd4b3094554506a3dfe6d4fd437f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12464ec10e57494fb8b7f9cd8c64b4e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "195afe943110489f87e24fd9ef8d8ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fbd4b3094554506a3dfe6d4fd437f9d",
            "placeholder": "​",
            "style": "IPY_MODEL_8404a4d2f50d43cba9892172d8c52da6",
            "value": " 2/2 [00:16&lt;00:00,  7.64s/it]"
          }
        },
        "323bf4164fd6471c8e5cf48c610fa170": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c69dc4b2a34595aebf997528fe56fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efd5530494e24f04a4e1844e6199d5f7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12464ec10e57494fb8b7f9cd8c64b4e8",
            "value": 2
          }
        },
        "4da186d5e8fc4ad8a9f225826903a86e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c856dc10f12e459da10697816ea714f8",
            "placeholder": "​",
            "style": "IPY_MODEL_d81e95ab245d4d51af0fcba0a2e29f6e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6b19f67279c149819f0938485b1ef88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4da186d5e8fc4ad8a9f225826903a86e",
              "IPY_MODEL_40c69dc4b2a34595aebf997528fe56fb",
              "IPY_MODEL_195afe943110489f87e24fd9ef8d8ac0"
            ],
            "layout": "IPY_MODEL_323bf4164fd6471c8e5cf48c610fa170"
          }
        },
        "8404a4d2f50d43cba9892172d8c52da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c856dc10f12e459da10697816ea714f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d81e95ab245d4d51af0fcba0a2e29f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efd5530494e24f04a4e1844e6199d5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
